{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural-Network-and-Deep-Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgU096nLj9uu",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks and Deep Learning\n",
        "\n",
        "This notebook is a self-recap notes for the 1st course \"Neural Networks and Deep Learning\" out of 5 courses within the \"Deep Learning Specialization\" on coursera.\n",
        "\n",
        "## Week1\n",
        "\n",
        "### Why is Deep Learning taking off?\n",
        "\n",
        "![alt text](https://miro.medium.com/max/2000/1*yhUYvpYxAjmMp3V9jGnttg.jpeg)\n",
        "\n",
        "To achieve high performance, two factors are often needed: \n",
        "\n",
        "* build large NNs \n",
        "* prepared large amounts of labeled data.\n",
        "\n",
        "![alt text](https://cherrythecoder.files.wordpress.com/2018/07/s8-36.png?w=736)\n",
        "\n",
        "The success of today's deep learning rely not only on the data amount, but also on computation power, and appropriate training algorithms. From a iterative prototyping perspective, faster \"idea-code-experiment\" iteration produces faster feedback and hence improves products faster.\n",
        "\n",
        "## Week2\n",
        "### Binary Classification\n",
        "For each training sample $(x,y)$\n",
        "\n",
        "* feature space dimension $n_x$\n",
        "* number of training samples $m_{train}$, test samples $m_{test}$\n",
        "* input matrix (each col is a sample in this course) `X.shape =` $(n_x, m)$, label matrix (each col is a label) `Y.shape =` $(1, m)$\n",
        "\n",
        "### Logistic Regression\n",
        "$$\\hat{y} = \\sigma(\\mathbf{w}^\\intercal\\mathbf{x} + b)$$\n",
        "where\n",
        "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "The model coefficients $(\\mathbf{w}, b)$ are treated seperately in this course, instead of grouping them into $\\mathbf{\\theta} = (b, \\mathbf{w})$ with an additional feature row full of ones.\n",
        "\n",
        "One more notation, the superscript in parentheses $x^{(i)}$ denotes the i-th sample in the training set.\n",
        "\n",
        "#### Loss function\n",
        "$L(\\hat{y}, y) = \\frac{1}{2}(\\hat{y} - y)^2$ will give a non-convex optimization object function. So what we used in LR is\n",
        "\n",
        "$$-[y\\log\\hat{y} + (1-y)\\log(1-\\hat{y})]$$.\n",
        "\n",
        "#### Cost function\n",
        "A measure on whole dataset $$J(\\mathbf{w}, b) = \\frac{1}{m}\\sum L(\\hat{y}^{(i)}, y^{(i)})$$, which is to be minimized during training.\n",
        "\n",
        "#### Gradient descent\n",
        "\n",
        "$$\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\frac{\\partial J(\\mathbf{w}, b)}{\\partial\\mathbf{w}}$$\n",
        "\n",
        "$$b \\leftarrow b - \\alpha \\frac{\\partial J(\\mathbf{w}, b)}{\\partial b}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b5-YmIGjoEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# forward prop\n",
        "\n",
        "# a ---5-------- \n",
        "#               \\\n",
        "# b              \\\n",
        "#  \\               v=a+u ---11--- j=3v ---33\n",
        "#   3            /\n",
        "#    \\          /\n",
        "#     u=bc --6--\n",
        "#    /\n",
        "#   2\n",
        "#  /\n",
        "# c\n",
        "# ```\n",
        "\n",
        "# back prop\n",
        "\n",
        "# a ---1-------- \n",
        "#               \\\n",
        "# b              \\\n",
        "#  \\               v=a+u ---3--- j=3v ---1\n",
        "#   2            /\n",
        "#    \\          /\n",
        "#     u=bc --1--\n",
        "#    /\n",
        "#   3\n",
        "#  /\n",
        "# c\n",
        "# ```\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmTSGOHwj8ou",
        "colab_type": "text"
      },
      "source": [
        "## Week3\n",
        "### Computing a Neural Network's Output\n",
        "Notation\n",
        "\n",
        "For one node, $z = \\mathbf{w}^\\intercal \\mathbf{x} + b$ and activation $a = \\sigma(z)$.\n",
        "\n",
        "$a_i^{[l]}$ where $[l]$ is the layer index (input is indexed as 0), and $i$ is the node index in its layer.\n",
        "\n",
        "Similarly $\\mathbf{w}_i^{[l]\\intercal}$ is the weight vector to calculate node [l], i. Going further, we can stack these column vector $\\mathbf{w}$ horizontally to have a weight matrix with num_rows = num_outputs and num_cols = num_inputs.\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1200/1*buxOnswsinejx2FVZDuF8w.png)\n",
        "\n",
        "### Backpropagation intuition\n",
        "\n",
        "Considering one training sample,\n",
        "\n",
        "\"z-step\": $\\mathbf{z} = \\mathbf{Wx + b}$\n",
        "\n",
        "\"a-step\": $\\mathbf{y} = g(\\mathbf{z})$\n",
        "\n",
        "To infer gradients about the parameter, fist calculate $d\\mathbf{z}$\n",
        "\n",
        "$$d\\mathbf{z} = d\\mathbf{y} * g'(\\mathbf{z})$$\n",
        "\n",
        "Then \n",
        "\n",
        "$$d\\mathbf{W} = (d\\mathbf{z})(\\mathbf{x^\\intercal})$$\n",
        "$$d\\mathbf{b} = d\\mathbf{z}$$\n",
        "\n",
        "It is easier to chain rule on \"dz\"s as follows.\n",
        "\n",
        "Calculate  $d\\mathbf{z}^{[1]}$ from $d\\mathbf{z}^{[2]}$\n",
        "\n",
        "$$\\frac{dL}{d\\mathbf{z}^{[1]}} = \\frac{dL}{d\\mathbf{x}} * \\frac{d\\mathbf{x}}{d\\mathbf{z}^{[1]}}$$\n",
        "\n",
        "$$ d\\mathbf{z}^{[1]} = W^{[2]\\intercal}d\\mathbf{z}^{[2]} * g^{[1]'}(\\mathbf{z}^{[1]})$$\n",
        "\n",
        "![alt text](https://sandipanweb.files.wordpress.com/2017/11/grad_summary.png?w=676)\n",
        "\n",
        "### Random Initialization\n",
        "\n",
        "Initialize weights randomly since symmetry nodes are effectly equivalent to one nodes since they always update with same amount. Usually a small perturbation is reasonable since large weights might push to the small-gradient region. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6smChLycYLM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}